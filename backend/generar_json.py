import os
import sys

# Agregar el directorio backend al path para importar m√≥dulos
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.embedding_utils import procesar_documentos, leer_documento, dividir_por_secciones, extraer_autor, extraer_autor_texto
from app.vectorstore import add_text_chunk, save_index
import json

def procesar_y_entrenar_documentos(directorio: str = "uploads"):
    """
    Procesa todos los documentos en el directorio uploads y los agrega al sistema de vectores.
    Compatible con el nuevo sistema de chunks y FAISS.
    """
    
    if not os.path.exists(directorio):
        print(f"‚ùå El directorio {directorio} no existe")
        return
    
    archivos_pdf = [f for f in os.listdir(directorio) if f.lower().endswith('.pdf')]
    archivos_txt = [f for f in os.listdir(directorio) if f.lower().endswith('.txt')]
    
    if not archivos_pdf and not archivos_txt:
        print(f"‚ùå No se encontraron archivos PDF o TXT en {directorio}")
        return
    
    print(f"üìÅ Procesando {len(archivos_pdf)} archivos PDF y {len(archivos_txt)} archivos TXT...")
    
    total_chunks = 0
    documentos_procesados = []
    
    # Procesar archivos PDF
    for archivo in archivos_pdf:
        ruta_completa = os.path.join(directorio, archivo)
        print(f"üìÑ Procesando: {archivo}")
        
        try:
            # Leer el documento
            texto = leer_documento(ruta_completa)
            
            # Extraer metadatos
            autor = extraer_autor(ruta_completa) or extraer_autor_texto(texto)
            
            # Agregar informaci√≥n del t√≠tulo del documento
            add_text_chunk(archivo, f"T√≠tulo del documento: {archivo}")
            if autor:
                add_text_chunk(archivo, f"Autor del documento: {autor}")
            
            # Dividir en secciones
            secciones = dividir_por_secciones(texto)
            
            # Agregar cada secci√≥n como chunk
            chunks_archivo = 0
            for seccion in secciones:
                contenido = seccion.get("contenido", "")
                if contenido.strip():  # Solo procesar secciones no vac√≠as
                    add_text_chunk(archivo, contenido)
                    chunks_archivo += 1
            
            total_chunks += chunks_archivo
            documentos_procesados.append({
                "archivo": archivo,
                "tipo": "PDF",
                "chunks": chunks_archivo,
                "autor": autor
            })
            
            print(f"   ‚úÖ {chunks_archivo} chunks procesados")
            
        except Exception as e:
            print(f"   ‚ùå Error procesando {archivo}: {str(e)}")
    
    # Procesar archivos TXT
    for archivo in archivos_txt:
        ruta_completa = os.path.join(directorio, archivo)
        print(f"üìÑ Procesando: {archivo}")
        
        try:
            # Leer archivo de texto
            with open(ruta_completa, "r", encoding="utf-8") as f:
                texto = f.read()
            
            # Extraer metadatos
            autor = extraer_autor_texto(texto)
            
            # Agregar informaci√≥n del t√≠tulo del documento
            add_text_chunk(archivo, f"T√≠tulo del documento: {archivo}")
            if autor:
                add_text_chunk(archivo, f"Autor del documento: {autor}")
            
            # Dividir en secciones
            secciones = dividir_por_secciones(texto)
            
            # Agregar cada secci√≥n como chunk
            chunks_archivo = 0
            for seccion in secciones:
                contenido = seccion.get("contenido", "")
                if contenido.strip():  # Solo procesar secciones no vac√≠as
                    add_text_chunk(archivo, contenido)
                    chunks_archivo += 1
            
            total_chunks += chunks_archivo
            documentos_procesados.append({
                "archivo": archivo,
                "tipo": "TXT",
                "chunks": chunks_archivo,
                "autor": autor
            })
            
            print(f"   ‚úÖ {chunks_archivo} chunks procesados")
            
        except Exception as e:
            print(f"   ‚ùå Error procesando {archivo}: {str(e)}")
    
    # Guardar el √≠ndice FAISS
    save_index()
    
    # Generar reporte de procesamiento
    reporte = {
        "timestamp": str(datetime.now()),
        "total_documentos": len(documentos_procesados),
        "total_chunks": total_chunks,
        "documentos": documentos_procesados,
        "directorio": directorio
    }
    
    # Guardar reporte en JSON
    with open("reporte_procesamiento.json", "w", encoding="utf-8") as f:
        json.dump(reporte, f, ensure_ascii=False, indent=2)
    
    # Mostrar resumen
    print(f"\nüéâ PROCESAMIENTO COMPLETADO")
    print(f"üìä Total documentos: {len(documentos_procesados)}")
    print(f"üìä Total chunks: {total_chunks}")
    print(f"üíæ √çndice FAISS actualizado")
    print(f"üìÑ Reporte guardado en: reporte_procesamiento.json")
    
    return reporte

def verificar_estado_vectorstore():
    """Verifica el estado actual del vectorstore"""
    from app.vectorstore import faiss_index
    from app.db import Session, chunks_table
    
    print("üîç VERIFICANDO ESTADO DEL VECTORSTORE")
    
    # Verificar FAISS
    if faiss_index:
        print(f"üìä Vectores en FAISS: {faiss_index.ntotal}")
    else:
        print("‚ùå √çndice FAISS no encontrado")
    
    # Verificar base de datos
    db = Session()
    try:
        total_chunks = db.execute("SELECT COUNT(*) FROM chunks").scalar()
        print(f"üìä Chunks en base de datos: {total_chunks}")
        
        # Mostrar fuentes √∫nicas
        fuentes = db.execute("SELECT DISTINCT source FROM chunks").fetchall()
        print(f"üìä Documentos √∫nicos: {len(fuentes)}")
        for fuente in fuentes[:10]:  # Mostrar solo los primeros 10
            print(f"   - {fuente[0]}")
        
        if len(fuentes) > 10:
            print(f"   ... y {len(fuentes) - 10} m√°s")
            
    finally:
        db.close()

def limpiar_vectorstore():
    """Limpia completamente el vectorstore (usar con precauci√≥n)"""
    respuesta = input("‚ö†Ô∏è ¬øEst√°s seguro de que quieres limpiar TODA la base de vectores? (escribir 'SI' para confirmar): ")
    
    if respuesta != "SI":
        print("‚ùå Operaci√≥n cancelada")
        return
    
    from app.vectorstore import faiss_index, VECTOR_INDEX_PATH
    from app.db import Session, chunks_table
    
    # Limpiar base de datos
    db = Session()
    try:
        db.execute(chunks_table.delete())
        db.commit()
        print("‚úÖ Base de datos limpiada")
    finally:
        db.close()
    
    # Recrear √≠ndice FAISS vac√≠o
    import faiss
    from app.config import EMBED_DIM
    
    new_index = faiss.IndexFlatL2(EMBED_DIM)
    faiss.write_index(new_index, VECTOR_INDEX_PATH)
    print("‚úÖ √çndice FAISS recreado")

if __name__ == "__main__":
    from datetime import datetime
    
    print("ü§ñ GENERADOR DE EMBEDDINGS PARA IA CONTABLE")
    print("=" * 50)
    
    if len(sys.argv) > 1:
        comando = sys.argv[1].lower()
        
        if comando == "procesar":
            directorio = sys.argv[2] if len(sys.argv) > 2 else "uploads"
            procesar_y_entrenar_documentos(directorio)
            
        elif comando == "verificar":
            verificar_estado_vectorstore()
            
        elif comando == "limpiar":
            limpiar_vectorstore()
            
        else:
            print("‚ùå Comando no reconocido")
            print("Uso:")
            print("  python generar_json.py procesar [directorio]")
            print("  python generar_json.py verificar")
            print("  python generar_json.py limpiar")
    else:
        # Ejecutar procesamiento por defecto
        procesar_y_entrenar_documentos("uploads")